{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                            [.4, .5, .6],\n",
    "                            [.7, .8, .9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6563, 0.8255, 0.5258],\n",
       "        [0.6115, 0.7007, 0.3042],\n",
       "        [0.8216, 0.5399, 0.8989]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand_like(target)\n",
    "# This means the final scalar will be differentiate by x.\n",
    "x.requires_grad = True\n",
    "# You can get gradient of x, after differentiation.\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1119, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss: 6.7667e-02\n",
      "tensor([[0.5327, 0.6865, 0.4756],\n",
      "        [0.5645, 0.6561, 0.3699],\n",
      "        [0.7946, 0.5977, 0.8992]], requires_grad=True)\n",
      "2-th Loss: 4.0934e-02\n",
      "tensor([[0.4365, 0.5784, 0.4366],\n",
      "        [0.5280, 0.6214, 0.4211],\n",
      "        [0.7736, 0.6427, 0.8994]], requires_grad=True)\n",
      "3-th Loss: 2.4763e-02\n",
      "tensor([[0.3617, 0.4943, 0.4063],\n",
      "        [0.4995, 0.5944, 0.4608],\n",
      "        [0.7572, 0.6776, 0.8995]], requires_grad=True)\n",
      "4-th Loss: 1.4980e-02\n",
      "tensor([[0.3036, 0.4289, 0.3826],\n",
      "        [0.4774, 0.5734, 0.4917],\n",
      "        [0.7445, 0.7048, 0.8996]], requires_grad=True)\n",
      "5-th Loss: 9.0619e-03\n",
      "tensor([[0.2583, 0.3780, 0.3643],\n",
      "        [0.4602, 0.5571, 0.5158],\n",
      "        [0.7346, 0.7260, 0.8997]], requires_grad=True)\n",
      "6-th Loss: 5.4819e-03\n",
      "tensor([[0.2232, 0.3385, 0.3500],\n",
      "        [0.4468, 0.5444, 0.5345],\n",
      "        [0.7269, 0.7424, 0.8998]], requires_grad=True)\n",
      "7-th Loss: 3.3162e-03\n",
      "tensor([[0.1958, 0.3077, 0.3389],\n",
      "        [0.4364, 0.5346, 0.5491],\n",
      "        [0.7209, 0.7552, 0.8998]], requires_grad=True)\n",
      "8-th Loss: 2.0061e-03\n",
      "tensor([[0.1745, 0.2838, 0.3302],\n",
      "        [0.4283, 0.5269, 0.5604],\n",
      "        [0.7163, 0.7652, 0.8999]], requires_grad=True)\n",
      "9-th Loss: 1.2136e-03\n",
      "tensor([[0.1579, 0.2652, 0.3235],\n",
      "        [0.4220, 0.5209, 0.5692],\n",
      "        [0.7127, 0.7729, 0.8999]], requires_grad=True)\n",
      "10-th Loss: 7.3414e-04\n",
      "tensor([[0.1451, 0.2507, 0.3183],\n",
      "        [0.4171, 0.5163, 0.5760],\n",
      "        [0.7099, 0.7789, 0.8999]], requires_grad=True)\n",
      "11-th Loss: 4.4411e-04\n",
      "tensor([[0.1351, 0.2394, 0.3142],\n",
      "        [0.4133, 0.5126, 0.5814],\n",
      "        [0.7077, 0.7836, 0.8999]], requires_grad=True)\n",
      "12-th Loss: 2.6866e-04\n",
      "tensor([[0.1273, 0.2307, 0.3111],\n",
      "        [0.4104, 0.5098, 0.5855],\n",
      "        [0.7060, 0.7873, 0.8999]], requires_grad=True)\n",
      "13-th Loss: 1.6252e-04\n",
      "tensor([[0.1212, 0.2238, 0.3086],\n",
      "        [0.4081, 0.5076, 0.5887],\n",
      "        [0.7046, 0.7901, 0.9000]], requires_grad=True)\n",
      "14-th Loss: 9.8315e-05\n",
      "tensor([[0.1165, 0.2185, 0.3067],\n",
      "        [0.4063, 0.5059, 0.5912],\n",
      "        [0.7036, 0.7923, 0.9000]], requires_grad=True)\n",
      "15-th Loss: 5.9475e-05\n",
      "tensor([[0.1128, 0.2144, 0.3052],\n",
      "        [0.4049, 0.5046, 0.5932],\n",
      "        [0.7028, 0.7940, 0.9000]], requires_grad=True)\n",
      "16-th Loss: 3.5978e-05\n",
      "tensor([[0.1100, 0.2112, 0.3040],\n",
      "        [0.4038, 0.5036, 0.5947],\n",
      "        [0.7022, 0.7953, 0.9000]], requires_grad=True)\n",
      "17-th Loss: 2.1765e-05\n",
      "tensor([[0.1078, 0.2087, 0.3031],\n",
      "        [0.4030, 0.5028, 0.5959],\n",
      "        [0.7017, 0.7964, 0.9000]], requires_grad=True)\n",
      "18-th Loss: 1.3166e-05\n",
      "tensor([[0.1060, 0.2068, 0.3025],\n",
      "        [0.4023, 0.5022, 0.5968],\n",
      "        [0.7013, 0.7972, 0.9000]], requires_grad=True)\n",
      "19-th Loss: 7.9648e-06\n",
      "tensor([[0.1047, 0.2053, 0.3019],\n",
      "        [0.4018, 0.5017, 0.5975],\n",
      "        [0.7010, 0.7978, 0.9000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold: # loss가 1e-5보다 작은 경우 stop\n",
    "    iter_cnt += 1\n",
    "    \n",
    "    loss.backward() # Calculate gradients.\n",
    "\n",
    "    x = x - learning_rate * x.grad\n",
    "    \n",
    "    # You don't need to aware this now.\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
